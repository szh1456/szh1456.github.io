---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

* I am Zihang Song, currently a third-year Ph.D. student at [School of Computer
Science and Electronic Engineering](https://www.surrey.ac.uk/department-electrical-electronic-engineering), [University of
Surrey](https://www.surrey.ac.uk/). I am a member of the [the Institute for Communications Systems (ICS), home of the 5G/6G Innovation Centre (5G/6GIC)](https://www.surrey.ac.uk/institute-communication-systems), under the supervision of [**Professor Yue Gao**]([https://www.cs.columbia.edu/~xia/](https://www.surrey.ac.uk/people/yue-gao)) and [**Professor Rahim Tafazolli**](https://www.surrey.ac.uk/people/rahim-tafazolli-freng). I received my bachelor's and master's degree from School of Physics at Beihang University.  


*I am a research assistant of the EPSRC project  [GHz bandwidth sensing (GBSense)](http://www.gbsense.net/).

<!-- * During my undergraduate study, I focused more on optimization and statistical signal processing in wireless communication. I am lucky to have been working at National Key Laboratory of Science and Technology on Communication, advised by [Professor Jun Wang](https://scholar.google.com.hk/citations?user=bOK-froAAAAJ&hl=zh-CN) and at Center for Real-time Adaptive Signal Processing, advised by [Professor Y. Rosa Zheng](https://www.lehigh.edu/~yrz218/) -->
                                                                                                                                                                                                                    
I am passionate about solving exciting and impactful real-world challenges. My research is mainly about turning everyday objects in to sensors,  **for sensing physical and physiological signals around humans and robots.**

I play with various modalities of signals from both software and hardware sides. I have designed and prototyped different practical systems  leveraging the latest technical advances (e.g., **Multimodal Deep Sensing, Mixed Reality/AR/VR, Humanoid Robot**) for  human motion teaching (soft flex/pressure sensors and camera @UbiComp'21),  human activity/behavior monitoring (computational fabrics @UbiComp'19; EMG and impedance sensors @UbiComp'21), cross-medium localization (laser light @MobiSys'22), and interactions (conductive threads @CHI'20). Feel free to contact me if interested in similar topics!


<!-- **I am actively seeking for a research intern position for summer 2021. Please ** -->

Recent News
======
* [06/2022] I graduated from Dartmouth with a master's degree (surprisingly) and is moving to Columbia with Xia. Will miss here Hanover!
* [05/2022] Started my research internship at Snap Research, working on reducing the motion-to-photon latency for enabling various cool interacvtive systems. Stay tuned!
* [03/2022] Our paper **"Sunflower: Locating Underwater Robots From the Air"** has been conditionally accepted to [MobiSys 2022](https://www.sigmobile.org/mobisys/2022/). The first system ever achieves wirelessly localizing underwater robots from the air withut additional infrastructure. Laser light is our secret for cross-medium sensing. Please check out our demo video [here!](https://www.youtube.com/watch?v=ofpqm2G2s_U)
* [09/2021] We are presenting both **ASLTeach** and **FaceSense** in [UbiComp 2021](https://www.ubicomp.org/ubicomp2021/)!
* [07/2021] Our COVID-motivated paper **"FaceSense: Sensing Face Touch with an Ear-worn System"** is accepted with minor revision by IMWUT (UbiComp2021). It's more than one-year-long effort collaborating with 4 universities. Cheers for the team's hard work during the pandemic!  Please check out our [paper](https://dl.acm.org/doi/pdf/10.1145/3478129) for more details. 
* [06/2021] Started my research internship at Signify (Philips Lighting), focusing on deep learning and sensor data fusion!
* [11/2020] Gave a guest lecture on next-generation mobile  platform -- computational fabrics in CS 69/169 at Dartmouth.
* [10/2020] Our paper **"Teaching American Sign Language in Mixed Reality"** was accepted by [IMWUT]((https://dl.acm.org/doi/10.1145/3432211)) (UbiComp2021). A great collaboration with researchers from cognitive science and education department at Dartmouth and sign language experts from Gallaudet University. This is our first work on teaching human motion at population scale without coaches. Check out the [presentation](https://www.youtube.com/watch?v=695M7eGxZJ4) for more details!
<!-- * [03/2020] I gave a demo and [talk](https://www.youtube.com/watch?v=lHfvueWdjJQ&t=6s) for our PolarTag paper **"PolarTag: Invisible Data with Light Polarization"** on [HotMobile 2020](http://www.hotmobile.org/2020/). Thanks for everyone's attention and vote! We won the <span style="color:red"> **Best Demo Award** </span>! -->


<!-- * [02/2020] Received the ACM HotMobile 2020 Student Travel Award. See you at Austin! -->
<!-- * [12/2019] One paper got accepted by **[HotMobile 2020](http://www.hotmobile.org/2020/)**.
* [12/2019] One paper got accepted by **[CHI 2020](https://chi2020.acm.org/)**.
* [09/2019] I presented our fabric paper **"Reconstructing Human Joint Motion with Computational Fabrics"** on **[UbiComp 2019](http://ubicomp.org/ubicomp2019/)** in London. -->




Visitors
=======
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=250&t=n&d=gkUgx_rJxyGnlm9h49vUyEn8lS4ZIy-1rPBbiEUZCKY&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>


